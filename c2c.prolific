User Flow: Container to Container Networking

## Assumptions 
- You have a CF deployed with silk release

## What

In the HTTP route track of stories we talked about the dataflow for HTTP traffic. But what if appA, an app within CloudFoundry, wants to talk to appB, another app within CloudFoundry? Well, before Container to Container networking (c2c) and wanted a shortcut from appA to appB, you were out of luck (unless you wanted to do some ill-advised hacking, we'll talk about this later). Without c2c, if appA wanted to talk to appB, then the traffic had to travel outside of the CF foundation, hit the external load balancer, and go through the normal HTTP traffic flow. 

```
Life Before C2C                                  +------+
                                                 |      |
                                                 |      |
          +------------------------------------+ | AppA |
          |                                      |      |
          |                                      |      |
          |                                      +------+
          |
          v                                      +------+
                            +-----------+        |      |
+------------------+        |HTTP Router|        |      |
|HTTP Load Balancer| +----> |(GoRouter) | +----> | AppB |
+------------------+        +-----------+        |      |
                                                 |      |
                                                 +------+

```

So many extra hops for apps within the same CF! Especially if the apps are on the same Diego Cell! Those hops come with extra latency. Also, even if appB  *only* needs to be accessed by appA (for example if appA is the frontend microservice and appB is the backend microservice for appA), appB still needs to be accessible via an HTTP route! This is exposing appB to more attack vectors than it should be.

With container to container networking (c2c) apps within a foundation can talk directly to each other. Now appA can talk to appB without leaving the CF foundation. Also, appB doesn't need to be accessible via an HTTP route (just an internal one, we'll get to that later).

```
Life with C2C

+------+        +------+
|      |        |      |
|      |        |      |
| AppA | +----> | AppB |
|      |        |      |
|      |        |      |
+------+        +------+
```

However, in order for appA to be *allowed* to talk to appB, we will need to create a network policy. 

Let's ignore how this is all setup for now and just go through the user workflow.

## How

1. Push appA.

1. Push appB with `--no-route` so that there is no HTTP route for appB.

1. Get the overlay IP for appB `cf ssh appB -c "env | grep CF_INSTANCE_INTERNAL_IP"`
   (Explantation what overlay is in future stories! For now just know that each app instance has a unique overlay IP that c2c uses.)

1. Get onto the container for appA and curl the appB internal IP and app port. 
   ```
cf ssh appA
watch  "curl CF_INSTANCE_INTERNAL_IP:8080"
   ```
   You should get a `Connection refused` error because there is no network policy yet.

1.  In another terminal, add a network policy from appA to appB, with protocol tcp, on port 8080. Run `cf add-network-policy --help` to figure out how to do this with the CLI.


### Expected Result
As soon as you add the policy, the curl from inside of the appA container to appB should succeed. If it doesn't work, check that you created the policy in the correct direction, from appA --> appB, not the other way around. Also make sure that appB uses app port 8080, if not, change the policy appropriately.  

## Resources

L: c2c

---

User Flow: Container to Container Networking with Service Discovery


## Assumptions 
- You have a CF deployed with silk release
- You have appA  talking to appB via c2c networking and policy (see previous story "User Flow Container to Container Networking")

## What 

In the previous story you were able to use c2c networking to get appA to talk directly to appB using the overlay IP for appB (explanation of what overlay is will come soon, I swear).

But IPs are ugly and URLs are pretty. Also, what happens when you restage an app? Also, how do I load balance across many instances when I can only use IPs?

In order to fix these problems, we implemented Service Discovery, which is apart of cf-networking-release. Service Discovery is also sometimes called app-sd. Service Discovery is a fancy way of saying we handle the URL -> IP translation for internal routes. Now appA can "discover" where the "service" appB is, without having to know the IP.

Service Discovery is implemented using "internal routes" these routes will *only* work from one CF app to another. They will not be accessible from clients outside of CF.

## How 

1. Start off where you left off from the previous story "User Flow Container to Container Networking"). You should have appA talking to appB via an overlay IP using `watch  "curl CF_INSTANCE_INTERNAL_IP:8080"` inside of the appA container in one terminal. 

1. In another terminal, run `cf restart appB`
   Predictably, the curl from appA to appB fails when appB is stopped. But it should come back when appB starts running again, right? ...Right? WHY IS IT STILL FAILING?

1. Recheck the overlay IP for appB `cf ssh appB -c "env | grep CF_INSTANCE_INTERNAL_IP"`
   What?! It moved! Use this new overlay IP to curl appB from appA and see that it still works. Lesson learned: IPs suck. Let's use Service Discovery instead.

1. When you create a route, any route, you have to supply a domain. To create an internal route, it must use an internal domain. We'll get into why in another story. For now, run `cf domains` and see that you should have a domain (or two) that is labeled `internal`.  Note the name of an internal domain that DOES NOT CONTAIN THE WORD "istio". You probably have the internal domain "apps.internal". Let's use that. (If you don't have a non istio internal domain, follow the resource at the bottom of this story to add a custom internal domain).

1. Using `cf map-route`, create and map a route for appB that uses the domain "apps.internal". May I suggest the route, appB.apps.internal?

1. In the terminal that is in the container for appA, run `watch  "curl appB.apps.internal:8080"`. 

1. Restart appB.

1. Scale appB. Can you tell which instance you are hitting?

### Expected Result
Now that you are using internal routes to communicate via c2c, it shouldn't matter that appB is restarted. As long as appB is running, appA should be able to access it thanks to Service Discovery. When there are multiple instances of appB, the internal route will automatically load balance between all of the instances. 

## Resources

https://github.com/cloudfoundry/cf-networking-release/blob/develop/docs/app-sd.md#internal-domains
https://github.com/cloudfoundry/cf-networking-release/blob/develop/docs/app-sd.md

L: c2c

---

User Flow: Create your own Internal Domain

## Assumptions 
- You have a CF deployed with silk release
- You have appA  talking to appB via c2c networking and policy (see previous story "User Flow: Container to Container Networking with Service Discovery")

## What 

In the previous story you created an internal route for appA to talk to appB using the domain "apps.internal", but what if we wanted to create our own internal domain (may I suggest meow.meow.meow)?

## How 

1. Start off where you left off from the previous story "User Flow Container to Container Networking"). You should have appA talking to appB via an overlay IP using `watch  "curl appB.apps.internal:8080"` inside of the appA container in one terminal. 

1. In another terminal, create a new internal domain `cf create-shared-domain meow.meow.meow --internal`
   Check that it worked
   ```
$ cf domains
Getting domains in org o as admin...
name                         status   type   details
meow.meow.meow               shared          internal
   ```

1. Using `cf map-route`, create and map a route for appB that uses our new internal domain "meow.meow.meow". May I suggest the route, appB.meow.meow.meow?

1. In the terminal that is in the container for appA, use this new internal route to curl appB `watch "curl appB.meow.meow.meow:8080"`
    What? "Could not resolve host"???? Why doesn't it work like our other internal route? Unlike other domains, internal domains require one more step in order for them to work.

1. Download the manifest for your CF. Look at the property `internal_domains` on the `bosh_dns_adapter` job. It probably looks like this: 
   ``` 
internal_domains:
      - apps.internal.
   ```

   So unfortunately, there is a deploy time dependency for internal domains. I know, this makes me sad too. Let's dig in why this is.

   Warning, Architecture Description Ahead (please follow along with the diagram below): When an app makes ANY network request that requires DNS lookup (that is, any request to a URL, not an IP) , the DNS lookup first hits Bosh DNS. Bosh DNS then checks to see if the domain of the URL being requested matches any of the internal domains that it knows about from the Bosh DNS Adapter `internal_domains` property. There is no reason why this couldn't be dynamic, Bosh DNS Adapter *could* make an API call to CAPI to figure out what the up-to-date internal domains are. But it doesn't, so it's not dynamic. Then the Bosh DNS Adapter calls out to the Service Discovery Controller, which keeps track of what internal route maps to what overlay IP, very similar to the routes table in the GoRouter.


   ![href](https://github.com/cloudfoundry/cf-networking-release/blob/develop/docs/architecture-diagram.png?raw=true)

1. Add our internal domain meow.meow.meow to the bosh manifest. 

1. Redeploy your environment. 

1. In the terminal that is in the container for appA, use this new internal route to curl appB `watch "curl appB.meow.meow.meow:8080"`

### Expected Result

appA should be able to successfully reach appB using the internal route with our brand new internal domain. 

## Resources

https://github.com/cloudfoundry/cf-networking-release/blob/develop/docs/app-sd.md#internal-domains
https://github.com/cloudfoundry/cf-networking-release/blob/develop/docs/app-sd.md


L: c2c

---

Break things with Internal Domains!

## Assumptions

## What 
In the previous story ("User Flow: Create your own Internal Domain") we talked about how Bosh DNS redirects all DNS lookups where the request matches an internal domain to the Bosh DNS Adapter. In this story we are going to exploit this.

   ![href](https://github.com/cloudfoundry/cf-networking-release/blob/develop/docs/architecture-diagram.png?raw=true)

## How 

**Pretend you are innocent user1**
1. Push [proxy app](https://github.com/cloudfoundry/cf-networking-release/tree/develop/src/example-apps/proxy) and call it appA. 

1. Make sure appA has an http route, let's call it APPA_ROUTE.

1. From your terminal, use appA's `/proxy` endpoint to send traffic from appA to neopets.com 
   `watch "curl APPA_ROUTE/proxy/neopets.com"`
   You should get back some html for neopets! Fun! :D

**Now pretend you are malicious user2**
As a malicious actor, you know that appA is sending traffic to neopets.com. You want to break their app and make it so appA can't reach neopets. You can do this by shadowing the neopets.com domain with an internal domain. 

1. Create the internal domain `neopets.com` (look at `cf create-shared-domain --help` if you don't remember how) 

   Instead of adding this new internal domain to the bosh manifest and redeploying, we are going to hack this in on the Diego Cell. This is a great, fast, ( and dangerous) debugging technique. 
1. Ssh onto the Diego Cell where appA is running and become root.

1. You need to find what config file holds the information you want to change. The config on the VMs does not directly match the bosh manifest. To find any config for any bosh job on CF you can find it at `/var/vcap/jobs/JOB_NAME`. There are many files there for Bosh DNS Adapter. In order to figure out exactly what you need to change, look at [Bosh DNS Job in the release code](https://github.com/cloudfoundry/cf-networking-release/tree/develop/jobs/bosh-dns-adapter) and look where the `internal_domains` property is used. [hint](https://github.com/cloudfoundry/cf-networking-release/blob/develop/jobs/bosh-dns-adapter/templates/handlers.json.erb#L11). [hint](https://github.com/cloudfoundry/cf-networking-release/blob/develop/jobs/bosh-dns-adapter/spec#L10).

1. Edit the correct config file and add an entry for our new domain `neopets.com`. It should look exactly like `apps.internal` except for the name. 

1. Now you'll need to restart the Bosh DNS Adapter process so that it will run with our new config. Linux Bosh VMs use monit as a process manager. Run `monit summary` to see all of the processes running on this VM. Restart the Bosh DNS Adapter by running `monit restart bosh-dns-adapter`. Keep running `monit summary` until the Bosh DNS Adapter is successfully running again. If it fails to start, then you probably made a syntax error in the config file. Look at the logs and fix the error.

1. You will also need to `monit restart bosh-dns-adapter`

**Back to pretending you are innocent user1**

1. From your terminal, use appA's `/proxy` endpoint to send traffic from appA to neopets.com
   `watch "curl APPA_ROUTE/proxy/neopets.com"`
   Where did neopets go???


### Expected Result
AppA should no longer be able to access neopets. :(

## Questions

1. Neopets is a silly example. What is a worse example that customers could run into?
1. What permissions does a user require to exploit this?
1. Would you consider this a security concern?
1. Would your assessment change if internal domains were dynamic and didn't require setting a bosh property at deploy time? Why or why not?

## Resources
https://github.com/cloudfoundry/cf-networking-release/blob/develop/docs/app-sd.md#internal-domains
https://github.com/cloudfoundry/cf-networking-release/blob/develop/docs/app-sd.md

L: c2c

---

Overlay vs Underlay
L: c2c
---

A closer look at c2c policies - iptables and marks
L: c2c
---

[RELEASE] Container to Container Networking ⇧
L: c2c
